{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "a-HwxN4Iyg_j",
        "colab_type": "code",
        "outputId": "a840ca07-da5c-4a44-a8a6-46bdb85e5c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Apr 16 09:52:14 2019\n",
        "\n",
        "@author: karm2204\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#####         https://chrisorm.github.io/VAE-pyt.html\n",
        "#             Variational Autoencoder in Pytorch\n",
        "#             https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py\n",
        "#             https://github.com/bobchennan/VAE_NBP/blob/master/vae_dp.py\n",
        "#             https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import argparse\n",
        "from torchvision.datasets import utils\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn.modules import upsampling\n",
        "from torch.functional import F\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_printoptions(precision=8)\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "batch_size = 128\n",
        "learning_rate = 31e-4\n",
        "image_size = 784\n",
        "h_dim = 400\n",
        "z_dim = 100\n",
        "num_epochs = 20\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "def get_data_loader(dataset_location, batch_size):\n",
        "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "    # start processing\n",
        "    def lines_to_np_array(lines):\n",
        "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
        "    splitdata = []\n",
        "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
        "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
        "        filepath = os.path.join(dataset_location, filename)\n",
        "        utils.download_url(URL + filename, dataset_location)\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "        x = lines_to_np_array(lines).astype('float32')\n",
        "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
        "        # pytorch data loader\n",
        "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
        "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
        "        splitdata.append(dataset_loader)\n",
        "    return splitdata\n",
        "\n",
        "train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", batch_size)\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "for x in train_loader:\n",
        "    plt.imshow(x[0, 0])\n",
        "    break\n",
        "    \n",
        "torch.cuda.memory_allocated(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n",
            "Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n",
            "Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n45eaMn6pKeOXmjJ+qan/B9JVumBrfh42AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sLyvHhhGKshB",
        "colab_type": "code",
        "outputId": "94a213c7-7ef2-411b-dd25-04040ae1fd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "next(enumerate(train_loader))[1].size()\n",
        "torch.cuda.memory_allocated(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "GUfU7LW-1Jut",
        "colab_type": "code",
        "outputId": "3184d76b-0098-4228-b860-d68730ee4ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=100):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "#         self.encoder = nn.Sequential(nn.Linear(image_size, h_dim),\\\n",
        "#                                 nn.ReLU()\\\n",
        "#                                )\n",
        "        self.encoder= nn.Sequential(nn.Conv2d(1,32,3),\\\n",
        "                                    nn.ELU(),\\\n",
        "                                    nn.AvgPool2d(2,2),\\\n",
        "                                    nn.Conv2d(32,64,3),\\\n",
        "                                    nn.ELU(),\\\n",
        "                                    nn.AvgPool2d(2,2),\\\n",
        "                                    nn.Conv2d(64,256,5),\\\n",
        "                                    nn.ELU(),\\\n",
        "                                    )\n",
        "#         self.fc_1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc_1 = nn.Linear(256, z_dim)\n",
        "        self.fc_2 = nn.Linear(256, z_dim)\n",
        "\n",
        "        self.fc_3 = nn.Linear(z_dim, 256)\n",
        "#         self.fc_5 = nn.Linear(h_dim, image_size)\n",
        "        self.decoder= nn.Sequential(nn.ELU(),\\\n",
        "                                    nn.Conv2d(256,64,5, padding = 4),\\\n",
        "                                    nn.ELU(),\\\n",
        "                                    nn.UpsamplingBilinear2d(scale_factor=2),\\\n",
        "                                    nn.Conv2d(64, 32, 3, padding=2),\\\n",
        "                                    nn.ELU(),\\\n",
        "                                    nn.UpsamplingBilinear2d(scale_factor=2),\\\n",
        "                                    nn.Conv2d(32, 16, 3, padding=2),\\\n",
        "                                    nn.ELU(),\\\n",
        "                                    nn.Conv2d(16, 1, 3, padding=2),\\\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "        \n",
        "    \"\"\" Encode a batch of samples, and return posterior parameters for each point.\"\"\"    \n",
        "    def encode(self, x):\n",
        "#         h_1 = F.relu(self.fc_1(x))\n",
        "      h_1 = self.encoder(x)\n",
        "      return self.fc_1(h_1.view(-1,256)), self.fc_2(h_1.view(-1,256))\n",
        "\n",
        "\n",
        "    \"\"\" Reparameterisation trick to sample z values. \n",
        "        This is stochastic during training,  and returns the mode during evaluation.\n",
        "        For each training sample (we get 128 batched at a time)\n",
        "        - take the current learned mu, stddev for each of the z_dim \n",
        "        (in the pytorch VAE example, this is 20, z_dim = 20)\n",
        "          dimensions and draw a random sample from that distribution\n",
        "        - the whole network is trained so that these randomly drawn\n",
        "          samples decode to output that looks like the input\n",
        "        - which will mean that the std, mu will be learned\n",
        "          *distributions* that correctly encode the inputs\n",
        "        - due to the additional KLD term (see loss_function() below)\n",
        "          the distribution will tend to unit Gaussians\n",
        "        Parameters\n",
        "        ----------\n",
        "        mu : [128, z_dim] mean matrix\n",
        "        logvar : [128, z_dim] variance matrix\n",
        "        Returns\n",
        "        -------\n",
        "        During training random sample from the learned ZDIMS-dimensional\n",
        "        normal distribution; during inference its mean.\n",
        "        \"\"\"\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(logvar/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "\n",
        "    \"\"\" Decode a batch of latent variables\"\"\"\n",
        "    def decode(self, z):\n",
        "    #        h_3 = F.relu(self.fc_4(z))\n",
        "    #        return F.sigmoid(self.fc_5(h_3))\n",
        "        h_2 = self.fc_3(z).view(-1,256,1,1)\n",
        "        h_3 = self.decoder(h_2)\n",
        "        return h_3\n",
        "\n",
        "\n",
        "    \"\"\" Takes a batch of samples, encodes them, and then decodes them again to compare.\"\"\"\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "torch.cuda.memory_allocated(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ywvLbrAtzHaM",
        "colab_type": "code",
        "outputId": "557d429b-82ae-43ee-f52b-f6006b27c6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2330
        }
      },
      "cell_type": "code",
      "source": [
        "#%%\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\"\"\" ELBO assuming entries of x are binary variables, with closed form KLD.\"\"\"\n",
        "def loss_function(x_reconst, x, mu, logvar):\n",
        "    bce = F.binary_cross_entropy(x_reconst, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    # Normalise by same number of elements as in reconstruction\n",
        "    # KLD /= x.view(-1, image_size).data.shape[0] * image_size\n",
        "    return bce + KLD\n",
        "#%%\n",
        "\n",
        "# ----------\n",
        "#  Train\n",
        "# ----------\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    log_interval = 100\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset), 100. *\n",
        "                batch_idx / len(train_loader), -loss.item() / len(data)\n",
        "            ))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "        epoch, -train_loss / len(train_loader.dataset)))\n",
        "\n",
        "#%%\n",
        "\n",
        "# ----------\n",
        "#  Test\n",
        "# ----------\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    # ind = np.arange(x.shape[0])\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(valid_loader):\n",
        "            data = data.to(device)\n",
        "#           test_data = torch.from_numpy(test_data[np.random.choice(ind, size=batch_size)])\n",
        "#           test_data = Variable(test_data, requires_grad=False)\n",
        "            reconst_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(reconst_batch, data, mu, logvar).item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 28)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                        reconst_batch.view(128, 1, 28, 28)[:n]])\n",
        "#                 save_image(comparison.cpu(),\n",
        "#                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "#            print(data.view(batch_size, 2,2)[:n])\n",
        "#            print(reconst_batch.view(batch_size, 2,2)[:n])\n",
        "            \n",
        "            \n",
        "    test_loss /= len(valid_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(-test_loss))\n",
        "        \n",
        "#%%    \n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, 21):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        # 64 sets of random z_dim-float vectors, i.e. 64 locations / MNIST\n",
        "        # digits in latent space\n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(128, z_dim).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "#             save_image(sample.view(64, 1, 28, 28),\n",
        "#                        'results/sample_' + str(epoch) + '.png')\n",
        "    torch.save(model.state_dict(), 'vae.pth')            \n",
        "            \n",
        "torch.cuda.memory_allocated(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([128, 784])) that is different to the input size (torch.Size([128, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\t Loss: -538.268188\n",
            "Train Epoch: 1 [12800/50000 (26%)]\t Loss: -185.493027\n",
            "Train Epoch: 1 [25600/50000 (51%)]\t Loss: -154.845123\n",
            "Train Epoch: 1 [38400/50000 (77%)]\t Loss: -131.645767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([80, 784])) that is different to the input size (torch.Size([80, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 Average loss: -175.0758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([16, 784])) that is different to the input size (torch.Size([16, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: -125.6572\n",
            "Train Epoch: 2 [0/50000 (0%)]\t Loss: -128.689240\n",
            "Train Epoch: 2 [12800/50000 (26%)]\t Loss: -115.358040\n",
            "Train Epoch: 2 [25600/50000 (51%)]\t Loss: -110.359077\n",
            "Train Epoch: 2 [38400/50000 (77%)]\t Loss: -113.802322\n",
            "====> Epoch: 2 Average loss: -114.5521\n",
            "====> Test set loss: -111.1518\n",
            "Train Epoch: 3 [0/50000 (0%)]\t Loss: -108.954338\n",
            "Train Epoch: 3 [12800/50000 (26%)]\t Loss: -107.931015\n",
            "Train Epoch: 3 [25600/50000 (51%)]\t Loss: -108.284012\n",
            "Train Epoch: 3 [38400/50000 (77%)]\t Loss: -102.166962\n",
            "====> Epoch: 3 Average loss: -106.8470\n",
            "====> Test set loss: -105.2445\n",
            "Train Epoch: 4 [0/50000 (0%)]\t Loss: -103.175385\n",
            "Train Epoch: 4 [12800/50000 (26%)]\t Loss: -103.013214\n",
            "Train Epoch: 4 [25600/50000 (51%)]\t Loss: -103.561813\n",
            "Train Epoch: 4 [38400/50000 (77%)]\t Loss: -103.904861\n",
            "====> Epoch: 4 Average loss: -103.3368\n",
            "====> Test set loss: -102.6411\n",
            "Train Epoch: 5 [0/50000 (0%)]\t Loss: -104.225105\n",
            "Train Epoch: 5 [12800/50000 (26%)]\t Loss: -99.050484\n",
            "Train Epoch: 5 [25600/50000 (51%)]\t Loss: -101.705658\n",
            "Train Epoch: 5 [38400/50000 (77%)]\t Loss: -102.316673\n",
            "====> Epoch: 5 Average loss: -101.3975\n",
            "====> Test set loss: -101.9183\n",
            "Train Epoch: 6 [0/50000 (0%)]\t Loss: -103.753815\n",
            "Train Epoch: 6 [12800/50000 (26%)]\t Loss: -98.465347\n",
            "Train Epoch: 6 [25600/50000 (51%)]\t Loss: -97.563339\n",
            "Train Epoch: 6 [38400/50000 (77%)]\t Loss: -97.548424\n",
            "====> Epoch: 6 Average loss: -99.7451\n",
            "====> Test set loss: -100.8551\n",
            "Train Epoch: 7 [0/50000 (0%)]\t Loss: -102.008331\n",
            "Train Epoch: 7 [12800/50000 (26%)]\t Loss: -101.002411\n",
            "Train Epoch: 7 [25600/50000 (51%)]\t Loss: -98.065559\n",
            "Train Epoch: 7 [38400/50000 (77%)]\t Loss: -95.481834\n",
            "====> Epoch: 7 Average loss: -98.8995\n",
            "====> Test set loss: -98.8062\n",
            "Train Epoch: 8 [0/50000 (0%)]\t Loss: -95.123535\n",
            "Train Epoch: 8 [12800/50000 (26%)]\t Loss: -102.878448\n",
            "Train Epoch: 8 [25600/50000 (51%)]\t Loss: -91.696854\n",
            "Train Epoch: 8 [38400/50000 (77%)]\t Loss: -94.880615\n",
            "====> Epoch: 8 Average loss: -98.1673\n",
            "====> Test set loss: -99.2467\n",
            "Train Epoch: 9 [0/50000 (0%)]\t Loss: -96.984253\n",
            "Train Epoch: 9 [12800/50000 (26%)]\t Loss: -99.390434\n",
            "Train Epoch: 9 [25600/50000 (51%)]\t Loss: -96.809982\n",
            "Train Epoch: 9 [38400/50000 (77%)]\t Loss: -98.871582\n",
            "====> Epoch: 9 Average loss: -97.5797\n",
            "====> Test set loss: -98.2539\n",
            "Train Epoch: 10 [0/50000 (0%)]\t Loss: -94.366928\n",
            "Train Epoch: 10 [12800/50000 (26%)]\t Loss: -101.407639\n",
            "Train Epoch: 10 [25600/50000 (51%)]\t Loss: -94.781120\n",
            "Train Epoch: 10 [38400/50000 (77%)]\t Loss: -95.736252\n",
            "====> Epoch: 10 Average loss: -97.1263\n",
            "====> Test set loss: -98.2725\n",
            "Train Epoch: 11 [0/50000 (0%)]\t Loss: -98.508659\n",
            "Train Epoch: 11 [12800/50000 (26%)]\t Loss: -96.396240\n",
            "Train Epoch: 11 [25600/50000 (51%)]\t Loss: -93.153366\n",
            "Train Epoch: 11 [38400/50000 (77%)]\t Loss: -97.880905\n",
            "====> Epoch: 11 Average loss: -96.6934\n",
            "====> Test set loss: -98.0395\n",
            "Train Epoch: 12 [0/50000 (0%)]\t Loss: -95.163391\n",
            "Train Epoch: 12 [12800/50000 (26%)]\t Loss: -96.486618\n",
            "Train Epoch: 12 [25600/50000 (51%)]\t Loss: -97.734215\n",
            "Train Epoch: 12 [38400/50000 (77%)]\t Loss: -92.013031\n",
            "====> Epoch: 12 Average loss: -96.3399\n",
            "====> Test set loss: -96.8083\n",
            "Train Epoch: 13 [0/50000 (0%)]\t Loss: -98.094688\n",
            "Train Epoch: 13 [12800/50000 (26%)]\t Loss: -95.416962\n",
            "Train Epoch: 13 [25600/50000 (51%)]\t Loss: -93.915649\n",
            "Train Epoch: 13 [38400/50000 (77%)]\t Loss: -96.193771\n",
            "====> Epoch: 13 Average loss: -96.0908\n",
            "====> Test set loss: -96.9013\n",
            "Train Epoch: 14 [0/50000 (0%)]\t Loss: -96.663773\n",
            "Train Epoch: 14 [12800/50000 (26%)]\t Loss: -97.620056\n",
            "Train Epoch: 14 [25600/50000 (51%)]\t Loss: -98.526535\n",
            "Train Epoch: 14 [38400/50000 (77%)]\t Loss: -98.078232\n",
            "====> Epoch: 14 Average loss: -95.7382\n",
            "====> Test set loss: -95.9142\n",
            "Train Epoch: 15 [0/50000 (0%)]\t Loss: -92.766464\n",
            "Train Epoch: 15 [12800/50000 (26%)]\t Loss: -96.463249\n",
            "Train Epoch: 15 [25600/50000 (51%)]\t Loss: -94.720932\n",
            "Train Epoch: 15 [38400/50000 (77%)]\t Loss: -96.001602\n",
            "====> Epoch: 15 Average loss: -95.5320\n",
            "====> Test set loss: -97.5906\n",
            "Train Epoch: 16 [0/50000 (0%)]\t Loss: -97.409653\n",
            "Train Epoch: 16 [12800/50000 (26%)]\t Loss: -91.866478\n",
            "Train Epoch: 16 [25600/50000 (51%)]\t Loss: -98.566330\n",
            "Train Epoch: 16 [38400/50000 (77%)]\t Loss: -95.098770\n",
            "====> Epoch: 16 Average loss: -95.3957\n",
            "====> Test set loss: -97.1306\n",
            "Train Epoch: 17 [0/50000 (0%)]\t Loss: -95.650970\n",
            "Train Epoch: 17 [12800/50000 (26%)]\t Loss: -98.520111\n",
            "Train Epoch: 17 [25600/50000 (51%)]\t Loss: -94.828217\n",
            "Train Epoch: 17 [38400/50000 (77%)]\t Loss: -91.287064\n",
            "====> Epoch: 17 Average loss: -95.0929\n",
            "====> Test set loss: -96.3645\n",
            "Train Epoch: 18 [0/50000 (0%)]\t Loss: -95.982376\n",
            "Train Epoch: 18 [12800/50000 (26%)]\t Loss: -97.180725\n",
            "Train Epoch: 18 [25600/50000 (51%)]\t Loss: -92.672600\n",
            "Train Epoch: 18 [38400/50000 (77%)]\t Loss: -94.832939\n",
            "====> Epoch: 18 Average loss: -94.9518\n",
            "====> Test set loss: -96.3354\n",
            "Train Epoch: 19 [0/50000 (0%)]\t Loss: -95.808182\n",
            "Train Epoch: 19 [12800/50000 (26%)]\t Loss: -93.932816\n",
            "Train Epoch: 19 [25600/50000 (51%)]\t Loss: -92.644409\n",
            "Train Epoch: 19 [38400/50000 (77%)]\t Loss: -96.335632\n",
            "====> Epoch: 19 Average loss: -94.7900\n",
            "====> Test set loss: -95.4527\n",
            "Train Epoch: 20 [0/50000 (0%)]\t Loss: -96.356941\n",
            "Train Epoch: 20 [12800/50000 (26%)]\t Loss: -91.414658\n",
            "Train Epoch: 20 [25600/50000 (51%)]\t Loss: -93.379913\n",
            "Train Epoch: 20 [38400/50000 (77%)]\t Loss: -96.578163\n",
            "====> Epoch: 20 Average loss: -94.5657\n",
            "====> Test set loss: -95.7257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16423936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ilFxgXH4-yTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Question 2.1\n",
        "def importance_sampling_minibatch(model, images, samples):\n",
        "  K = 200\n",
        "  hidden_size = 100\n",
        "  minibatch_size = 128\n",
        "  image_dim = 28\n",
        "  image_size = 784\n",
        "  images.to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    mean_val = torch.mean(samples, dim = 1)\n",
        "    sigma_val = torch.std(samples, dim = 1)\n",
        "\n",
        "    #two distributions, one standard and one displaced \n",
        "    standard = torch.distributions.normal.Normal(torch.zeros(min(minibatch_size,mean_val.size(0)),hidden_size).cuda(),\\\n",
        "                                                 torch.ones(min(minibatch_size,mean_val.size(0)),hidden_size).cuda())\n",
        "    displaced = torch.distributions.normal.Normal(mean_val, sigma_val)\n",
        "\n",
        "    #sample z x 200 from displaced distribution(mean and s.d. from encoder run on data)\n",
        "    samples = [displaced.sample(sample_shape=torch.Size()) for i in range(K)]\n",
        "\n",
        "    #p and q are (200, 128,100)\n",
        "    #density values for each samples, for each minibatch, for each uncorrelated z_i\n",
        "    log_p_samples = torch.stack([standard.log_prob(samples[i].data) for i in range(len(samples))])\n",
        "    log_q_samples = torch.stack([displaced.log_prob(samples[i].data) for i in range(len(samples))])\n",
        "\n",
        "    #torch.Size([200, 128, 784]) reconstructions of each sample/minibatch/image\n",
        "    reconstructions = torch.stack([model.decode(samples[i].data).view(-1,image_size) for i in range(K)])\n",
        "\n",
        "    #p of x given z computed with Binary cross entropy \n",
        "    log_p_x_given_z = ((images.view(-1,image_size) * torch.log(torch.clamp(reconstructions.data, 1e-9, 1))) + \\\n",
        "                       ((1.0-images.view(-1,image_size))*torch.log(torch.clamp(1.0-reconstructions.data, 1e-9, 1))))\n",
        "     \n",
        "    #max prob is 1, so max log prob is log(200) for logsumexp trick: \n",
        "    return (-np.log(K) + torch.logsumexp(torch.sum(log_p_x_given_z, dim=-1) + torch.sum(log_p_samples, dim=-1) - \\\n",
        "                                         torch.sum(log_q_samples, dim=-1), 0))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgpNleoGry9R",
        "colab_type": "code",
        "outputId": "90e4782f-11ef-4544-aeaf-99987f3b6994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "cell_type": "code",
      "source": [
        "#Generating own samples for proof that importance_sampling_minibatch works properly\n",
        "temp_images = next(enumerate(valid_loader))[1].view(-1,784).cuda()\n",
        "temp_mu, temp_sigma = model.encode(temp_images.view(-1,1,28,28))\n",
        "temp_sigma = torch.exp(temp_sigma.data/2)\n",
        "\n",
        "standard = torch.distributions.normal.Normal(torch.zeros(128,100).cuda(), torch.ones(128,100).cuda())\n",
        "displaced = torch.distributions.normal.Normal(temp_mu, temp_sigma)\n",
        "samples = [displaced.sample(sample_shape=torch.Size()) for i in range(200)]\n",
        "samples = torch.stack(samples).permute(1,0,2)\n",
        "\n",
        "importance_sampling_minibatch(model, temp_images, samples)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -92.31223297,  -63.27921677, -113.73207092,  -70.89508820,\n",
              "         -85.37721252, -100.38253784, -102.74098206,  -69.94426727,\n",
              "         -46.26094055,  -60.02542114,  -94.82479095,  -93.93408203,\n",
              "        -100.73024750,  -93.77836609,  -72.96858215,  -83.02745056,\n",
              "         -53.77232742,  -73.77635956, -113.08279419,  -89.57521820,\n",
              "         -73.94335175,  -87.49971771,  -97.65991974, -101.83093262,\n",
              "         -98.01223755,  -53.07160187,  -92.80167389,  -46.71390152,\n",
              "        -113.44345093,  -84.89527130,  -43.09423447, -122.96604156,\n",
              "         -73.05554199, -109.36096954,  -52.45328903,  -95.82112122,\n",
              "        -101.18788910,  -94.53914642,  -55.81551361, -109.71980286,\n",
              "         -96.80652618, -102.81462860, -103.22021484,  -95.43441772,\n",
              "         -81.02103424,  -98.11636353,  -96.92123413, -135.13087463,\n",
              "        -114.37727356,  -85.13206482,  -75.71747589,  -41.29470062,\n",
              "        -102.78981018,  -60.13010406,  -65.13063812,  -82.14741516,\n",
              "         -40.08565140, -109.04063416,  -74.51987457,  -69.37487793,\n",
              "         -94.06162262,  -67.97039795, -115.80542755,  -99.15568542,\n",
              "         -66.13281250,  -79.80606842,  -86.66410828,  -82.38892365,\n",
              "         -94.38711548, -100.54125977,  -79.71794891, -107.21059418,\n",
              "         -50.20274353,  -68.80497742,  -45.37063217, -112.74115753,\n",
              "         -77.85655975,  -43.34794998,  -81.91979218,  -67.84381866,\n",
              "         -99.40105438, -131.05867004, -112.29495239,  -89.15450287,\n",
              "         -94.30663300,  -89.40067291,  -93.69071198,  -84.08082581,\n",
              "         -76.28330231,  -88.12690735, -119.23210907,  -79.92389679,\n",
              "         -92.28515625, -115.45419312,  -93.70643616,  -97.29908752,\n",
              "         -76.01664734,  -74.93844604,  -60.64663315, -109.16982269,\n",
              "        -134.67277527, -101.33275604,  -72.29828644,  -88.85060120,\n",
              "         -91.63651276, -110.87655640,  -84.73065948,  -95.52154541,\n",
              "         -82.59838104,  -87.33431244,  -70.14966583,  -72.26007843,\n",
              "         -93.61067963,  -88.27375793,  -72.19729614,  -95.23828125,\n",
              "         -66.04872131,  -63.52269363,  -78.27109528, -103.61350250,\n",
              "         -83.26838684,  -88.71061707,  -95.87657166,  -74.55075073,\n",
              "        -124.18788910, -105.88092041, -104.75679016,  -68.02568054],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "AMT_isebFmLC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For more practical purposes, the following importance sampling function was used instead since it generates its own samples $z_{ik}$."
      ]
    },
    {
      "metadata": {
        "id": "1SNlHGb6Fk82",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def importance_sampling(trained_model, image_loader, minibatch_size, image_size, hidden_size, K):\n",
        "  \n",
        "  log_px = []\n",
        "  with torch.no_grad():\n",
        "\n",
        "    #forward pass through model with image set\n",
        "    for i, data in enumerate(image_loader):\n",
        "      data = data.to(device)\n",
        "      mean_val, log_var_val = model.encode(data)\n",
        "      images = data\n",
        "    #   mean_val_test = mean_val[0].data\n",
        "    #   log_var_val_test = log_var_val[0].data\n",
        "      sigma_val = torch.exp(log_var_val.data/2)\n",
        "\n",
        "      #two distributions, one standard and one displaced \n",
        "      standard = torch.distributions.normal.Normal(torch.zeros(min(minibatch_size,mean_val.size(0)),hidden_size).cuda(),\\\n",
        "                                                   torch.ones(min(minibatch_size,mean_val.size(0)),hidden_size).cuda())\n",
        "      displaced = torch.distributions.normal.Normal(mean_val, sigma_val)\n",
        "\n",
        "      #sample z x 200 from displaced distribution(mean and s.d. from encoder run on data)\n",
        "      #sample = displaced.sample(sample_shape=torch.Size())\n",
        "      samples = [displaced.sample(sample_shape=torch.Size()) for i in range(K)]\n",
        "\n",
        "      #p and q are (200, 128,100)\n",
        "      #density values for each samples, for each minibatch, for each uncorrelated z_i\n",
        "      log_p_samples = torch.stack([standard.log_prob(samples[i].data) for i in range(len(samples))])\n",
        "      log_q_samples = torch.stack([displaced.log_prob(samples[i].data) for i in range(len(samples))])\n",
        "\n",
        "      #torch.Size([200, 128, 784]) reconstructions of each sample/minibatch/image\n",
        "      reconstructions = torch.stack([model.decode(samples[i].data).view(-1,image_size) for i in range(K)])\n",
        "\n",
        "      #p of x given z computed with Binary cross entropy ################# REVIEW DIMENSIONS HERE #####################\n",
        "      log_p_x_given_z = ((images.view(-1,image_size) * torch.log(torch.clamp(reconstructions.data, 1e-9, 1))) + \\\n",
        "                         ((1.0-images.view(-1,image_size))*torch.log(torch.clamp(1.0-reconstructions.data, 1e-9, 1))))\n",
        "\n",
        "      log_px.append( (-np.log(K) + torch.logsumexp(torch.sum(log_p_x_given_z, dim=-1) + torch.sum(log_p_samples, dim=-1) - torch.sum(log_q_samples, dim=-1), 0)))\n",
        "\n",
        "    return torch.cat(log_px)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oL5ukEKLHKmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e9b5765c-c934-4992-c63b-6b53c3b14f02"
      },
      "cell_type": "code",
      "source": [
        "#Question 2.2\n",
        "valid_log_px = importance_sampling(model, valid_loader, 128, 784, 100, 200)\n",
        "test_log_px = importance_sampling(model, test_loader, 128, 784, 100, 200)\n",
        "\n",
        "print('validation set log-likelihood estimate: ', torch.mean(valid_log_px).cpu().numpy())\n",
        "print('test set log-likelihood estimate: ', torch.mean(test_log_px).cpu().numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation set log-likelihood estimate:  -88.63752\n",
            "test set log-likelihood estimate:  -88.14166\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}